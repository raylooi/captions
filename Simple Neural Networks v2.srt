1
00:00:00,000 --> 00:00:04,000
Let's now take a look at how neural networks actually work.

2
00:00:04,000 --> 00:00:06,600
In general, the simplest version of the neural network

3
00:00:06,600 --> 00:00:08,920
has one layer, and you can think of it

4
00:00:08,920 --> 00:00:14,240
as multiple regression algorithms running simultaneously,

5
00:00:14,240 --> 00:00:16,800
one for each of the classes we're trying to predict.

6
00:00:16,800 --> 00:00:18,840
So here's a simple case where we're

7
00:00:18,840 --> 00:00:21,440
trying to take the same data, which

8
00:00:21,440 --> 00:00:27,200
will be our x inputs plus the set to 1 bias.

9
00:00:27,200 --> 00:00:30,480
We will, of course, have weights times each of these.

10
00:00:30,480 --> 00:00:32,880
And there is, of course, going to be

11
00:00:32,880 --> 00:00:40,480
a weight from each of the inputs to each of our activation units

12
00:00:40,480 --> 00:00:42,800
or essentially regression algorithms.

13
00:00:42,800 --> 00:00:45,480
Essentially, this one will predict class 3.

14
00:00:45,480 --> 00:00:46,680
This one will predict class 2.

15
00:00:46,680 --> 00:00:48,440
This one will predict class 1.

16
00:00:48,440 --> 00:00:51,000
And this will be using the familiar one

17
00:00:51,000 --> 00:00:54,200
versus rest configuration, where essentially each

18
00:00:54,600 --> 00:00:57,920
of these activation units gets trained

19
00:00:57,920 --> 00:01:02,000
to differentiate between things that are part of its class

20
00:01:02,000 --> 00:01:02,920
and are not.

21
00:01:02,920 --> 00:01:04,400
And the hope is that every instance

22
00:01:04,400 --> 00:01:08,080
will show up with exactly one value set, for instance,

23
00:01:08,080 --> 00:01:10,360
010, to predict class 2.

24
00:01:10,360 --> 00:01:13,560
This is the simplest form of network.

25
00:01:13,560 --> 00:01:15,360
It is single layer.

26
00:01:15,360 --> 00:01:18,240
And it's really just a generalization

27
00:01:18,240 --> 00:01:21,080
of the familiar techniques that we've seen for regression.

28
00:01:21,080 --> 00:01:23,400
We can train it much the same way.

29
00:01:23,440 --> 00:01:25,480
Of course, things can get more interesting

30
00:01:25,480 --> 00:01:29,560
in that we can create multilayer or deeper networks.

31
00:01:29,560 --> 00:01:31,560
When people talk about deep neural networks,

32
00:01:31,560 --> 00:01:33,160
that's really what they're saying.

33
00:01:33,160 --> 00:01:37,400
There are multiple so-called hidden layers in between.

34
00:01:37,400 --> 00:01:41,000
Now, the interesting thing about a deep neural network

35
00:01:41,000 --> 00:01:43,480
is that training, which, of course, involves

36
00:01:43,480 --> 00:01:49,920
putting inputs along here and giving feedback

37
00:01:50,000 --> 00:01:54,240
on the error or the loss with respect to predicted class

38
00:01:54,240 --> 00:01:57,960
versus actual class, this training will separately

39
00:01:57,960 --> 00:02:01,800
set weights for this individual activation unit,

40
00:02:01,800 --> 00:02:03,280
this one and this one.

41
00:02:03,280 --> 00:02:06,160
And ultimately, each of them will learn essentially

42
00:02:06,160 --> 00:02:09,480
to predict an intermediate feature,

43
00:02:09,480 --> 00:02:11,080
which will be different from the others

44
00:02:11,080 --> 00:02:13,960
because each of our activation units

45
00:02:13,960 --> 00:02:16,320
essentially starts with different random settings

46
00:02:16,320 --> 00:02:17,360
for the weights.

47
00:02:17,360 --> 00:02:19,840
And as you give feedback, they'll each differentiate

48
00:02:19,840 --> 00:02:22,040
in different ways that are complementary.

49
00:02:22,040 --> 00:02:24,880
So basically, the important thing here

50
00:02:24,880 --> 00:02:26,640
is that the deep neural network actually

51
00:02:26,640 --> 00:02:30,400
learns to extract meaningful features

52
00:02:30,400 --> 00:02:33,360
in these hidden or intermediate layers.

53
00:02:33,360 --> 00:02:37,080
Let's now go back to the case of the simple single layer

54
00:02:37,080 --> 00:02:40,000
network and look at how it works in more detail.

55
00:02:40,000 --> 00:02:42,200
So of course, as we already alluded to,

56
00:02:42,200 --> 00:02:46,200
you're going to be training multiple of these neurons,

57
00:02:46,200 --> 00:02:49,760
one for each class, on the same input.

58
00:02:49,760 --> 00:02:51,480
So let's take a look at how this actually

59
00:02:51,480 --> 00:02:53,080
works with an example.

60
00:02:53,080 --> 00:02:57,320
We're going to have two features, x1 and x2

61
00:02:57,320 --> 00:02:58,400
as our inputs.

62
00:02:58,400 --> 00:03:00,080
Of course, note that that also means

63
00:03:00,080 --> 00:03:03,280
we will have a bias x0 equals 1.

64
00:03:03,280 --> 00:03:06,440
And we're going to build essentially two predictors,

65
00:03:06,440 --> 00:03:08,640
one for the AND function.

66
00:03:08,640 --> 00:03:11,840
So it should learn x1 and x2.

67
00:03:11,840 --> 00:03:14,200
This is the Boolean function.

68
00:03:14,200 --> 00:03:15,840
And then we will have a second one,

69
00:03:15,840 --> 00:03:20,800
which learns the Boolean OR function, x1 or x2.

70
00:03:20,800 --> 00:03:25,400
We're going to use a simple function called

71
00:03:25,400 --> 00:03:29,160
HEAVYSIDE to be our activation function.

72
00:03:29,160 --> 00:03:42,120
And that's basically going to look like this, where

73
00:03:42,120 --> 00:03:44,520
it essentially gives you true or false,

74
00:03:44,520 --> 00:03:48,760
depending on whether x is less than 0 or greater than

75
00:03:48,760 --> 00:03:49,600
or equal to 0.

76
00:03:49,600 --> 00:03:51,040
So note that this one is actually

77
00:03:51,040 --> 00:03:53,000
a very particular function that usually we

78
00:03:53,000 --> 00:03:56,880
don't use much in real networks because it's not

79
00:03:56,880 --> 00:03:58,040
differentiable.

80
00:03:58,040 --> 00:04:00,360
But for our particular example, it

81
00:04:00,360 --> 00:04:04,560
makes for a simpler set of use cases.

82
00:04:04,560 --> 00:04:06,560
Now, usually for real neural networks,

83
00:04:06,560 --> 00:04:09,120
we're not going to use HEAVYSIDE because it's not really

84
00:04:09,120 --> 00:04:11,920
suitable for gradient descent because of its stair-step

85
00:04:11,920 --> 00:04:12,880
nature.

86
00:04:12,880 --> 00:04:15,240
But nonetheless, it makes for a convenient example

87
00:04:15,240 --> 00:04:16,840
to illustrate the principles.

88
00:04:16,840 --> 00:04:18,520
So we're going to use it here.

89
00:04:18,520 --> 00:04:23,440
The basic setup will then be the diagram that we see here.

90
00:04:23,440 --> 00:04:25,800
So again, we can see that we have the bias, which

91
00:04:25,800 --> 00:04:27,240
is also x0.

92
00:04:27,240 --> 00:04:28,720
That will be value 1.

93
00:04:28,720 --> 00:04:32,080
And then we are going to have a series of weights.

94
00:04:32,080 --> 00:04:34,760
So we can see there's a weight vector here

95
00:04:34,760 --> 00:04:40,480
that corresponds to our activation unit 1.

96
00:04:40,600 --> 00:04:43,720
And there similarly will be a weight vector

97
00:04:43,720 --> 00:04:48,560
along these lines, which will be for our second activation

98
00:04:48,560 --> 00:04:49,680
unit.

99
00:04:49,680 --> 00:04:59,800
And of course, what will happen is we will do x dot with w1

100
00:04:59,800 --> 00:05:06,240
star to be our first activation unit,

101
00:05:06,240 --> 00:05:14,160
and x dot w2 star to be our second activation unit.

102
00:05:14,160 --> 00:05:20,920
Each of these will be what we see in the sigma here.

103
00:05:20,920 --> 00:05:25,440
And then we'll apply our HEAVYSIDE function over each.

104
00:05:25,440 --> 00:05:29,840
So this first one corresponds to this.

105
00:05:29,840 --> 00:05:32,880
And we'll ultimately have to be x1 and x2.

106
00:05:32,880 --> 00:05:37,320
The second one corresponds to here.

107
00:05:37,320 --> 00:05:40,920
And we'll ultimately learn x1 or x2.

108
00:05:40,920 --> 00:05:42,640
OK, so how do we actually train?

109
00:05:42,640 --> 00:05:47,000
Well, we do this actually in a completely incremental way.

110
00:05:47,000 --> 00:05:49,600
Essentially, we're going to take each sample, which I'll just

111
00:05:49,600 --> 00:05:52,560
call x, although we might call it x sub i.

112
00:05:52,560 --> 00:05:55,840
And we'll make a prediction for class j

113
00:05:55,840 --> 00:06:01,400
by multiplying our input x by all of the wj's in a vector.

114
00:06:01,400 --> 00:06:04,320
And of course, then we apply our activation function,

115
00:06:04,320 --> 00:06:06,280
which in this case was a HEAVYSIDE.

116
00:06:06,280 --> 00:06:11,120
So for our particular unit 1 that

117
00:06:11,120 --> 00:06:18,320
tries to predict x1 and x2, if we're given x of 0, 1

118
00:06:18,320 --> 00:06:22,760
and weights like this, of course, what we first have to do

119
00:06:22,760 --> 00:06:25,760
is take our original x and add the bias.

120
00:06:25,760 --> 00:06:28,480
So now we get a vector 1, 0, 1.

121
00:06:28,480 --> 00:06:33,600
We multiply now our x prime by our weight vector.

122
00:06:33,600 --> 00:06:36,840
And we get a value of 0.11, which is greater than 0.

123
00:06:36,840 --> 00:06:41,280
So therefore, our prediction is y1 equals 1.

124
00:06:41,280 --> 00:06:43,640
Now immediately, we can train.

125
00:06:43,640 --> 00:06:49,240
We can update our w of j into the next round

126
00:06:49,240 --> 00:06:52,320
by multiplying our error.

127
00:06:52,320 --> 00:06:57,200
So in this case, y minus y hat times the value of x

128
00:06:57,200 --> 00:07:00,280
times our learning rate eta, which

129
00:07:00,280 --> 00:07:04,240
looks exactly like we saw with stochastic gradient descent.

130
00:07:04,240 --> 00:07:06,720
And we're going to repeat this process for many rounds

131
00:07:06,720 --> 00:07:08,240
or epochs.

132
00:07:08,240 --> 00:07:10,640
So let's take a look at training in action.

133
00:07:10,640 --> 00:07:12,120
Here, we're going to learn OR.

134
00:07:12,120 --> 00:07:15,000
So this is really the activation unit number 2

135
00:07:15,000 --> 00:07:17,360
from our example.

136
00:07:17,360 --> 00:07:20,880
The truth table, therefore, the actual outputs

137
00:07:20,880 --> 00:07:24,480
of the function for Boolean OR are as follows.

138
00:07:24,480 --> 00:07:26,080
0 OR OR is 0.

139
00:07:26,080 --> 00:07:28,560
Everything else is a 1.

140
00:07:28,560 --> 00:07:30,920
When we initialize our neural network,

141
00:07:30,920 --> 00:07:35,560
we're going to set the weights to a random set of values.

142
00:07:35,560 --> 00:07:40,720
So we're going to have weight 2, 0, weight 2, 1, and weight 2,

143
00:07:40,720 --> 00:07:41,640
2 in this case.

144
00:07:41,640 --> 00:07:46,360
Those are for our initial bias, our x1 and our x2.

145
00:07:46,360 --> 00:07:51,200
So this is our initial set of values.

146
00:07:51,200 --> 00:07:53,280
Again, recall that we have three of these

147
00:07:53,280 --> 00:07:58,080
because the 0th one, the initial one, is for the bias.

148
00:07:58,080 --> 00:08:01,160
OK, we now start with the first input,

149
00:08:01,160 --> 00:08:06,160
which will be the first entry from our truth table.

150
00:08:06,160 --> 00:08:09,060
So in this case, we're training on x equals 0, 0.

151
00:08:09,060 --> 00:08:10,440
Of course, the first thing we need

152
00:08:10,440 --> 00:08:16,720
to do is add our bias, so a 0th element with value 1.

153
00:08:16,720 --> 00:08:22,000
And now we can multiply x by w2 star, or w2.

154
00:08:22,000 --> 00:08:24,000
And if we do this, it'll turn out

155
00:08:24,000 --> 00:08:28,800
the product is actually 0 because we have two 0s here

156
00:08:28,800 --> 00:08:30,920
and one 0 here.

157
00:08:30,920 --> 00:08:35,040
Now, if we look at that, we can apply Heaviside to it.

158
00:08:35,040 --> 00:08:38,600
Actually, Heaviside on 0 is 1 because Heaviside

159
00:08:38,600 --> 00:08:42,040
returns 1 for anything greater than or equal to 0.

160
00:08:42,040 --> 00:08:44,560
So our y hat is 1.

161
00:08:44,560 --> 00:08:48,120
The actual value, the real value of y, should be 0.

162
00:08:48,120 --> 00:08:50,800
So clearly, we have an error.

163
00:08:50,800 --> 00:08:56,520
Now, to do our weight update, we compute delta of w2

164
00:08:56,520 --> 00:08:59,000
by taking the learning rate times

165
00:08:59,000 --> 00:09:02,640
the difference between the actual and predicted values

166
00:09:02,640 --> 00:09:05,720
times our value of y that we tested on.

167
00:09:05,720 --> 00:09:07,880
And so in this case, our learning rate

168
00:09:07,880 --> 00:09:15,000
is going to be 0.01 times 0 minus 1 times 1.00.

169
00:09:15,000 --> 00:09:19,080
And that will give us an update like this.

170
00:09:19,080 --> 00:09:24,600
So we can see that we're going to adjust the 0th entry right

171
00:09:24,600 --> 00:09:29,280
here by negative 0.1 and leave the others the same.

172
00:09:29,280 --> 00:09:32,080
We will immediately do this update because, again, we're

173
00:09:32,080 --> 00:09:34,960
essentially doing stochastic gradient descent here.

174
00:09:34,960 --> 00:09:38,400
So now we will have a new version of our weights.

175
00:09:38,400 --> 00:09:40,520
You can see the lighter blue color here

176
00:09:40,520 --> 00:09:42,240
indicates what was updated.

177
00:09:42,240 --> 00:09:44,160
Now we're going to train on the next value.

178
00:09:44,160 --> 00:09:45,640
So again, we pull it down.

179
00:09:45,640 --> 00:09:48,640
We extend it with a 1.

180
00:09:48,640 --> 00:09:49,680
We multiply.

181
00:09:49,680 --> 00:09:52,000
Now we get 0.69.

182
00:09:52,000 --> 00:09:55,120
The heavy side on 0.69 will, again, be 1.

183
00:09:55,120 --> 00:09:57,280
In this case, we have a 1 and a 1.

184
00:09:57,280 --> 00:09:58,680
So the error is 0.

185
00:09:58,680 --> 00:10:02,520
So the delta w2 will also be 0.

186
00:10:02,520 --> 00:10:04,520
So this is good.

187
00:10:04,520 --> 00:10:07,600
Now we go to the next training example.

188
00:10:07,600 --> 00:10:09,480
Again, we extend it.

189
00:10:09,480 --> 00:10:10,360
We multiply.

190
00:10:10,360 --> 00:10:13,040
Now we get negative 0.31.

191
00:10:13,040 --> 00:10:15,880
So heavy side on a negative will be 0.

192
00:10:15,880 --> 00:10:19,600
In contrast, the y value is 1.

193
00:10:19,600 --> 00:10:21,720
So now we have an error.

194
00:10:21,720 --> 00:10:25,760
So again, eta times the difference between y and y hat

195
00:10:25,760 --> 00:10:30,680
times our value of x will give us a weight update,

196
00:10:30,680 --> 00:10:31,800
as we see here.

197
00:10:31,800 --> 00:10:33,560
We will immediately apply it.

198
00:10:33,560 --> 00:10:37,880
Now we train on 1, 1, extend it with a 1.

199
00:10:37,880 --> 00:10:40,760
Do our product at 0.41.

200
00:10:40,760 --> 00:10:44,000
We now do our prediction, which is a 1.

201
00:10:44,000 --> 00:10:46,920
The actual value of y is actually a 1.

202
00:10:46,920 --> 00:10:48,840
So we compute our update.

203
00:10:48,840 --> 00:10:52,560
And in this case, there is nothing to change.

204
00:10:52,560 --> 00:10:55,320
And we repeat this process through multiple epochs

205
00:10:55,320 --> 00:11:00,480
until eventually we end up with the weights set

206
00:11:00,480 --> 00:11:02,760
to define this particular line.

207
00:11:02,760 --> 00:11:05,800
And essentially, everything to the right of that line

208
00:11:05,800 --> 00:11:06,960
will be predicted as true.

209
00:11:06,960 --> 00:11:09,720
Everything below will be predicted as false.

210
00:11:09,720 --> 00:11:12,960
This is now the OR function.

211
00:11:12,960 --> 00:11:13,440
Great.

212
00:11:13,440 --> 00:11:14,880
So we saw an example with OR.

213
00:11:14,880 --> 00:11:18,520
Of course, in parallel, we're doing the same thing with AND.

214
00:11:18,520 --> 00:11:21,680
And in general, what we learn from the perceptron

215
00:11:21,680 --> 00:11:24,160
is going to be some kind of linear decision boundary,

216
00:11:24,160 --> 00:11:26,440
much like with logistic regression.

217
00:11:26,440 --> 00:11:29,320
But here, we're going to predict simply the class, not

218
00:11:29,320 --> 00:11:30,680
a probability of the class.

219
00:11:30,680 --> 00:11:33,840
And it's guaranteed that with enough epochs,

220
00:11:33,840 --> 00:11:37,480
the perceptron or the single activation unit

221
00:11:37,480 --> 00:11:41,600
will converge as long as the classes are linearly separable.

222
00:11:41,600 --> 00:11:46,360
How do we actually train a perceptron or simple neural

223
00:11:46,360 --> 00:11:49,080
network like we saw in our examples?

224
00:11:49,080 --> 00:11:50,840
Essentially, we can do this in Scikit-learn

225
00:11:50,840 --> 00:11:53,440
just by calling linear model perceptron.

226
00:11:53,440 --> 00:11:55,640
As usual, we would fit to the training data

227
00:11:55,640 --> 00:11:57,360
and predict with the test data.

228
00:11:57,360 --> 00:11:59,600
And this works pretty smoothly.

229
00:11:59,600 --> 00:12:02,000
Now, perhaps surprisingly, this is

230
00:12:02,000 --> 00:12:04,160
one of the few neural networks capabilities

231
00:12:04,160 --> 00:12:06,040
that's built into Scikit-learn.

232
00:12:06,040 --> 00:12:08,720
The developers of Scikit-learn decided early on

233
00:12:08,720 --> 00:12:11,480
that they do not want to be an overall neural networks

234
00:12:11,480 --> 00:12:15,240
library and instead left that to other tool kits

235
00:12:15,240 --> 00:12:17,800
that we'll be talking about in a moment.

236
00:12:17,800 --> 00:12:20,160
Also, perhaps interestingly, Apache Spark

237
00:12:20,160 --> 00:12:25,120
only has very limited deep learning capabilities as well.

238
00:12:25,120 --> 00:12:28,240
So this brings us to a new family of tools, namely

239
00:12:28,240 --> 00:12:30,200
neural network frameworks.

240
00:12:30,200 --> 00:12:32,680
If you follow the AI tech space at all,

241
00:12:32,680 --> 00:12:35,520
you might have heard of Google's TensorFlow or perhaps

242
00:12:35,520 --> 00:12:38,280
Apache MXNet or PyTorch.

243
00:12:38,280 --> 00:12:39,740
All of these frameworks are actually

244
00:12:39,740 --> 00:12:41,700
quite similar in spirit.

245
00:12:41,700 --> 00:12:44,140
They allow you to define multiple layers

246
00:12:44,140 --> 00:12:45,980
in a neural network.

247
00:12:45,980 --> 00:12:47,700
And then from that, they generate

248
00:12:47,700 --> 00:12:51,700
graphs of linear algebra operators working on tensors,

249
00:12:51,700 --> 00:12:55,060
where tensors are essentially a generalization of me.

250
00:12:55,060 --> 00:12:58,100
This style of computation actually, in many ways,

251
00:12:58,100 --> 00:13:00,660
looks similar to Apache Spark's notion

252
00:13:00,660 --> 00:13:05,020
of relational algebra operators over large relation.

253
00:13:05,020 --> 00:13:07,420
Now, out of these different neural network frameworks,

254
00:13:07,420 --> 00:13:10,860
PyTorch has recently emerged as essentially the winner.

255
00:13:10,860 --> 00:13:13,100
It's the most popular, the most widely used,

256
00:13:13,100 --> 00:13:16,100
and the most widely supported.

257
00:13:16,100 --> 00:13:19,580
So indeed, we will be adopting PyTorch for this course.

258
00:13:19,580 --> 00:13:22,620
It is, after all, the preeminent toolkit today

259
00:13:22,620 --> 00:13:25,220
for deep learning, especially deep learning with GPUs.

260
00:13:25,220 --> 00:13:27,620
And essentially, when you write PyTorch code,

261
00:13:27,620 --> 00:13:30,820
it ultimately compiles this down into CUDA code

262
00:13:30,820 --> 00:13:33,900
that runs on NVIDIA GPUs or the equivalent

263
00:13:33,900 --> 00:13:36,700
for other processors.

264
00:13:36,780 --> 00:13:40,420
Now, PyTorch by itself is a single GPU package,

265
00:13:40,420 --> 00:13:43,580
but there is another library called Horovod,

266
00:13:43,580 --> 00:13:47,180
which is useful for distributing PyTorch computations

267
00:13:47,180 --> 00:13:51,740
across multiple computers and also across multiple GPUs.

268
00:13:51,740 --> 00:13:53,180
So we'll be taking a look at this

269
00:13:53,180 --> 00:13:55,380
as we try to scale our deep learning tech.

270
00:13:55,380 --> 00:13:57,740
Now, what does a PyTorch program actually look like?

271
00:13:57,740 --> 00:14:01,020
Well, here's a sketch to give you a sense of this.

272
00:14:01,020 --> 00:14:03,340
Almost always, you are going to define

273
00:14:03,380 --> 00:14:04,980
your neural network model

274
00:14:06,380 --> 00:14:10,340
by subclassing Torch neural network module.

275
00:14:10,340 --> 00:14:13,300
And essentially, you define a series of layers

276
00:14:13,300 --> 00:14:16,020
by making them into member variables

277
00:14:16,020 --> 00:14:20,140
and then assigning them using torch.nn.something or other,

278
00:14:20,140 --> 00:14:22,580
which we'll talk about in more detail.

279
00:14:22,580 --> 00:14:26,780
Sometimes you'll also define a forward propagation function,

280
00:14:26,780 --> 00:14:30,620
as we see here, where you take the input,

281
00:14:30,620 --> 00:14:32,220
which in this case is the input

282
00:14:32,220 --> 00:14:33,900
to the entire neural network,

283
00:14:33,900 --> 00:14:37,340
and then each layer computes an output

284
00:14:37,340 --> 00:14:41,220
based on its operations over the input.

285
00:14:41,220 --> 00:14:43,220
And that composes through multiple steps

286
00:14:43,220 --> 00:14:44,460
or layers through the neural network

287
00:14:44,460 --> 00:14:47,020
until you eventually return the result.

288
00:14:48,740 --> 00:14:50,220
The rest of a PyTorch program

289
00:14:50,220 --> 00:14:52,140
typically instantiates the model.

290
00:14:52,140 --> 00:14:56,420
It calls model.train to put the model into training mode.

291
00:14:56,420 --> 00:14:57,940
And then there's a loop,

292
00:14:57,980 --> 00:15:02,740
which takes a series of input instances from the data,

293
00:15:02,740 --> 00:15:05,220
iterates over it in many epochs,

294
00:15:05,220 --> 00:15:07,900
does a computation by applying the model

295
00:15:07,900 --> 00:15:10,700
to the data's X inputs,

296
00:15:10,700 --> 00:15:12,220
in other words, the training instance.

297
00:15:12,220 --> 00:15:13,780
That produces a series of outputs,

298
00:15:13,780 --> 00:15:15,380
which are the predictions.

299
00:15:15,380 --> 00:15:18,180
From that, we will take the outputs

300
00:15:18,180 --> 00:15:21,460
and diff them with the actual labels

301
00:15:21,460 --> 00:15:24,820
to get a loss after we apply a loss function.

302
00:15:24,820 --> 00:15:27,340
Then we backward propagate the error

303
00:15:27,340 --> 00:15:28,580
to adjust the weights.

304
00:15:28,580 --> 00:15:31,540
And we'll talk more about this very soon.

305
00:15:31,540 --> 00:15:34,180
So let's summarize what we've seen so far

306
00:15:34,180 --> 00:15:37,940
about single layer neural networks.

307
00:15:37,940 --> 00:15:42,220
In general, what we have is a vector of artificial neurons

308
00:15:42,220 --> 00:15:45,220
that are instantiated as a single layer.

309
00:15:45,220 --> 00:15:48,620
We can use this to produce one versus all classifiers

310
00:15:48,620 --> 00:15:51,620
where each output makes a separate prediction.

311
00:15:51,620 --> 00:15:54,100
And essentially we can train all of these

312
00:15:54,100 --> 00:15:56,380
different perceptrons simultaneously

313
00:15:56,420 --> 00:15:59,220
based on an update a lot like gradient descent

314
00:15:59,220 --> 00:16:01,220
with a learning rate.

315
00:16:01,220 --> 00:16:03,180
A perception is guaranteed to converge

316
00:16:03,180 --> 00:16:05,260
for linearly separable data.

317
00:16:05,260 --> 00:16:08,260
But of course, not all data is linearly separable.

318
00:16:08,260 --> 00:16:10,980
And that requires us to create multi-layer networks

319
00:16:10,980 --> 00:16:15,300
as we started to see in our example with PyTorch.

320
00:16:15,300 --> 00:16:17,580
There are lots of frameworks besides PyTorch,

321
00:16:17,580 --> 00:16:19,100
but it's going to be our focus.

322
00:16:19,100 --> 00:16:21,060
And we're gonna see more about how to use it

323
00:16:21,060 --> 00:16:22,900
over the next few lectures.

